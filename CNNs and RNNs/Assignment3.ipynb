{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data faster I have written the text with its corresponding lable to a csv file. <br>\n",
    "For Text Classification: I used the Sentiment polarity datasets where 2000 text files are lable as positive or negative polarity .<br>\n",
    "For Sentence Classification: I used the Subjectivity datasets which contains the rotten_imdb dataset consisting of 5000 subjective and 5000 objective sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing<br>\n",
    "For both Text and Sentence classification, <br>\n",
    "The data is split into 85% to 15% ratio of training set (X_train,Y_train) and testing set(X_test,Y_test).\n",
    "I have used the label encoder to vectorize the Labels and a tokeniser with max 5000 words to vectorize the text input.\n",
    "I have then padded the text vectors so that each vector has a constant length of 150.\n",
    "\n",
    "Models <br>\n",
    "CNN <br>\n",
    "The initial layer is the embedding layer. <br>\n",
    "second layer is a dorpout layer of 0.2<br>\n",
    "then I have applied a single channel convolutional network with relu activation function.<br>\n",
    "Followed by a maxpooling with pool size 4 and multi layer perceptron sigmoid activation as an output layer.<br>\n",
    "Binary cross entropy loss function is used as it is binary classification problem. Adam optimizer is used and accuracy metric is used.<br>\n",
    "\n",
    "RNN<br>\n",
    "First layer is the embedding layer.<br>\n",
    "Then I have implemented a bidirection LSTM layer.<br>\n",
    "Followed by the a multi layer perceptron with a layer of relu activation <br>\n",
    "then i have added a dropout layer of 0.2<br>\n",
    "I have added another multi layer perceptron sigmoid activation as an output layer. <br>\n",
    "Binary cross entropy loss function is used as it is binary classification problem. Adam optimizer is used and accuracy metric is used.<br>\n",
    "\n",
    "The Test Accuracies obtained are: <br>\n",
    "CNN:<br> \n",
    " Text classification : 77.0%<br> \n",
    " Sentence Classification : 88.0%<br>\n",
    "RNN:<br>\n",
    " Text classification : 77.0%<br> \n",
    " Sentence Classification : 89.67%<br>\n",
    "\n",
    "To improve these accuracies I used Glove vector embeddings. I applied glove vectore embedding for both sentence and text classification with CNN obtained the following test accuracies:<br>\n",
    "  Sentence classification : 90.93% and <br>\n",
    "  Text classification : 90.60% <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D,Bidirectional\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = pd.read_csv('reviewPolarityCorpus.csv')\n",
    "df = data_text.sample(frac=1).reset_index(drop=True)\n",
    "X_text = df[\"Text\"]\n",
    "Y_text = df[\"Lable\"]\n",
    "le = LabelEncoder()\n",
    "Y_text = le.fit_transform(Y_text)\n",
    "Y_text = Y_text.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_text,Y_text,test_size=0.15)\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(max_words, 100, input_length=max_len))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 150, 100)          500000    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 150, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 146, 64)           32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 36, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 598,165\n",
      "Trainable params: 598,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1360 samples, validate on 340 samples\n",
      "Epoch 1/7\n",
      "1360/1360 [==============================] - 5s 4ms/step - loss: 0.6933 - acc: 0.5037 - val_loss: 0.6921 - val_acc: 0.5118\n",
      "Epoch 2/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.6873 - acc: 0.5235 - val_loss: 0.6878 - val_acc: 0.6118\n",
      "Epoch 3/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.6458 - acc: 0.7368 - val_loss: 0.6699 - val_acc: 0.5618\n",
      "Epoch 4/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.5018 - acc: 0.7618 - val_loss: 0.5781 - val_acc: 0.7059\n",
      "Epoch 5/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.3291 - acc: 0.9184 - val_loss: 0.6005 - val_acc: 0.7294\n",
      "Epoch 6/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.1614 - acc: 0.9478 - val_loss: 0.5331 - val_acc: 0.7824\n",
      "Epoch 7/7\n",
      "1360/1360 [==============================] - 3s 2ms/step - loss: 0.0664 - acc: 0.9779 - val_loss: 0.7060 - val_acc: 0.7794\n",
      "Accuracy: 77.00%\n"
     ]
    }
   ],
   "source": [
    "model_conv = conv_model()\n",
    "model_conv.summary()\n",
    "model_conv.fit(sequences_matrix,Y_train,batch_size=128,epochs=7,\n",
    "          validation_split=0.2)\n",
    "scores = model_conv.evaluate(test_sequences_matrix, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentence = pd.read_csv('rottenImdbCorpus.csv')\n",
    "data_df = data_sentence.sample(frac=1).reset_index(drop=True)\n",
    "#data preprocessing\n",
    "X_sent = data_df[\"Text\"]\n",
    "Y_sent = data_df[\"Lable\"]\n",
    "le = LabelEncoder()\n",
    "Y_sent = le.fit_transform(Y_sent)\n",
    "Y_sent = Y_sent.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_sent,Y_sent,test_size=0.15)\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(max_words, 50, input_length=max_len))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 50)           250000    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 150, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 146, 64)           16064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 36, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 332,165\n",
      "Trainable params: 332,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/10\n",
      "6800/6800 [==============================] - 13s 2ms/step - loss: 0.6018 - acc: 0.6599 - val_loss: 0.3808 - val_acc: 0.8518\n",
      "Epoch 2/10\n",
      "6800/6800 [==============================] - 11s 2ms/step - loss: 0.2572 - acc: 0.9006 - val_loss: 0.2995 - val_acc: 0.8776\n",
      "Epoch 3/10\n",
      "6800/6800 [==============================] - 11s 2ms/step - loss: 0.1485 - acc: 0.9482 - val_loss: 0.3426 - val_acc: 0.8788\n",
      "Epoch 4/10\n",
      "6800/6800 [==============================] - 12s 2ms/step - loss: 0.0933 - acc: 0.9685 - val_loss: 0.3597 - val_acc: 0.8735\n",
      "Epoch 5/10\n",
      "6800/6800 [==============================] - 11s 2ms/step - loss: 0.0657 - acc: 0.9778 - val_loss: 0.4231 - val_acc: 0.8653\n",
      "Epoch 6/10\n",
      "6800/6800 [==============================] - 11s 2ms/step - loss: 0.0483 - acc: 0.9832 - val_loss: 0.5015 - val_acc: 0.8653\n",
      "Epoch 7/10\n",
      "6800/6800 [==============================] - 11s 2ms/step - loss: 0.0297 - acc: 0.9901 - val_loss: 0.5680 - val_acc: 0.8712\n",
      "Epoch 8/10\n",
      "6800/6800 [==============================] - 14s 2ms/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.6514 - val_acc: 0.8724\n",
      "Epoch 9/10\n",
      "6800/6800 [==============================] - 13s 2ms/step - loss: 0.0150 - acc: 0.9951 - val_loss: 0.6880 - val_acc: 0.8665\n",
      "Epoch 10/10\n",
      "6800/6800 [==============================] - 12s 2ms/step - loss: 0.0176 - acc: 0.9937 - val_loss: 0.7455 - val_acc: 0.8629\n",
      "Accuracy: 88.00%\n"
     ]
    }
   ],
   "source": [
    "model_conv = conv_model()\n",
    "model_conv.summary()\n",
    "model_conv.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2)\n",
    "scores = model_conv.evaluate(test_sequences_matrix, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Extending the approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_sent,Y_sent,test_size=0.15)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_sent)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix('glove.6B/glove.6B.300d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "# create model\n",
    "model_embedding = Sequential()\n",
    "model_embedding.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen))\n",
    "model_embedding.add(layers.Conv1D(64, 5, activation='relu'))\n",
    "model_embedding.add(layers.GlobalMaxPooling1D())\n",
    "model_embedding.add(layers.Dense(10, activation='relu'))\n",
    "model_embedding.add(layers.Dense(1, activation='sigmoid'))\n",
    "model_embedding.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 100, 100)          2300200   \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 96, 64)            32064     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,332,925\n",
      "Trainable params: 2,332,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8500 samples, validate on 1500 samples\n",
      "Epoch 1/10\n",
      "8500/8500 [==============================] - 36s 4ms/step - loss: 0.3228 - acc: 0.8642 - val_loss: 0.2481 - val_acc: 0.8953\n",
      "Epoch 2/10\n",
      "8500/8500 [==============================] - 34s 4ms/step - loss: 0.1368 - acc: 0.9536 - val_loss: 0.2923 - val_acc: 0.8893\n",
      "Epoch 3/10\n",
      "8500/8500 [==============================] - 33s 4ms/step - loss: 0.0434 - acc: 0.9881 - val_loss: 0.3001 - val_acc: 0.9073\n",
      "Epoch 4/10\n",
      "8500/8500 [==============================] - 33s 4ms/step - loss: 0.0083 - acc: 0.9988 - val_loss: 0.3735 - val_acc: 0.9087\n",
      "Epoch 5/10\n",
      "8500/8500 [==============================] - 33s 4ms/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.4140 - val_acc: 0.9107\n",
      "Epoch 6/10\n",
      "8500/8500 [==============================] - 33s 4ms/step - loss: 3.6392e-04 - acc: 1.0000 - val_loss: 0.4481 - val_acc: 0.9080\n",
      "Epoch 7/10\n",
      "8500/8500 [==============================] - 33s 4ms/step - loss: 1.3563e-04 - acc: 1.0000 - val_loss: 0.4727 - val_acc: 0.9087\n",
      "Epoch 8/10\n",
      "8500/8500 [==============================] - 33s 4ms/step - loss: 7.5774e-05 - acc: 1.0000 - val_loss: 0.4917 - val_acc: 0.9100\n",
      "Epoch 9/10\n",
      "8500/8500 [==============================] - 33s 4ms/step - loss: 4.3364e-05 - acc: 1.0000 - val_loss: 0.5156 - val_acc: 0.9100\n",
      "Epoch 10/10\n",
      "8500/8500 [==============================] - 33s 4ms/step - loss: 2.5513e-05 - acc: 1.0000 - val_loss: 0.5348 - val_acc: 0.9093\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.9093\n"
     ]
    }
   ],
   "source": [
    "model_embedding.summary()\n",
    "history = model_embedding.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model_embedding.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_embedding.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = pd.read_csv('reviewPolarityCorpus.csv')\n",
    "df = data_text.sample(frac=1).reset_index(drop=True)\n",
    "X_rnn_text = df[\"Text\"]\n",
    "Y_rnn_text = df[\"Lable\"]\n",
    "le = LabelEncoder()\n",
    "Y_rnn_text = le.fit_transform(Y_rnn_text)\n",
    "Y_rnn_text = Y_rnn_text.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_rnn_text,Y_rnn_text,test_size=0.15)\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = Bidirectional(LSTM(64))(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_19 (Embedding)     (None, 150, 50)           250000    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 128)               58880     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 342,161\n",
      "Trainable params: 342,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1360 samples, validate on 340 samples\n",
      "Epoch 1/7\n",
      "1360/1360 [==============================] - 10s 7ms/step - loss: 0.6937 - acc: 0.4801 - val_loss: 0.6931 - val_acc: 0.4735\n",
      "Epoch 2/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.6845 - acc: 0.5934 - val_loss: 0.6876 - val_acc: 0.4794\n",
      "Epoch 3/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.7931 - acc: 0.7066 - val_loss: 0.5886 - val_acc: 0.7735\n",
      "Epoch 4/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.4669 - acc: 0.8250 - val_loss: 0.6011 - val_acc: 0.6618\n",
      "Epoch 5/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.3005 - acc: 0.9015 - val_loss: 0.5855 - val_acc: 0.6794\n",
      "Epoch 6/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.2257 - acc: 0.9243 - val_loss: 0.4738 - val_acc: 0.8000\n",
      "Epoch 7/7\n",
      "1360/1360 [==============================] - 4s 3ms/step - loss: 0.1218 - acc: 0.9640 - val_loss: 0.4891 - val_acc: 0.7824\n",
      "Accuracy: 77.00%\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=7,\n",
    "          validation_split=0.2)\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "scores = model.evaluate(test_sequences_matrix, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentence = pd.read_csv('rottenImdbCorpus.csv')\n",
    "data_df = data_sentence.sample(frac=1).reset_index(drop=True)\n",
    "#data preprocessing\n",
    "X_rnn_sent = data_df[\"Text\"]\n",
    "Y_rnn_sent = data_df[\"Lable\"]\n",
    "le = LabelEncoder()\n",
    "Y_rnn_sent = le.fit_transform(Y_rnn_sent)\n",
    "Y_rnn_sent = Y_rnn_sent.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_rnn_sent,Y_rnn_sent,test_size=0.15)\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = Bidirectional(LSTM(64))(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_20 (Embedding)     (None, 150, 50)           250000    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 128)               58880     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 342,161\n",
      "Trainable params: 342,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/7\n",
      "6800/6800 [==============================] - 26s 4ms/step - loss: 0.4682 - acc: 0.7797 - val_loss: 0.3363 - val_acc: 0.8406\n",
      "Epoch 2/7\n",
      "6800/6800 [==============================] - 22s 3ms/step - loss: 0.2278 - acc: 0.9168 - val_loss: 0.2603 - val_acc: 0.8976\n",
      "Epoch 3/7\n",
      "6800/6800 [==============================] - 27s 4ms/step - loss: 0.1481 - acc: 0.9487 - val_loss: 0.2769 - val_acc: 0.8935\n",
      "Epoch 4/7\n",
      "6800/6800 [==============================] - 27s 4ms/step - loss: 0.1185 - acc: 0.9590 - val_loss: 0.2584 - val_acc: 0.8988\n",
      "Epoch 5/7\n",
      "6800/6800 [==============================] - 23s 3ms/step - loss: 0.0859 - acc: 0.9704 - val_loss: 0.2742 - val_acc: 0.9047\n",
      "Epoch 6/7\n",
      "6800/6800 [==============================] - 22s 3ms/step - loss: 0.0774 - acc: 0.9775 - val_loss: 0.3256 - val_acc: 0.9000\n",
      "Epoch 7/7\n",
      "6800/6800 [==============================] - 22s 3ms/step - loss: 0.0502 - acc: 0.9854 - val_loss: 0.3822 - val_acc: 0.8971\n",
      "Accuracy: 89.67%\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=7,\n",
    "          validation_split=0.2)\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "scores = model.evaluate(test_sequences_matrix, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # c. Extending the approaches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_rnn_text,Y_rnn_text,test_size=0.15)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_rnn_text)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix('glove.6B/glove.6B.300d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          2300200   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 64)            32064     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,332,925\n",
      "Trainable params: 2,332,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /anaconda2/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 8500 samples, validate on 1500 samples\n",
      "Epoch 1/10\n",
      "8500/8500 [==============================] - 34s 4ms/step - loss: 0.3172 - acc: 0.8612 - val_loss: 0.2755 - val_acc: 0.8833\n",
      "Epoch 2/10\n",
      "8500/8500 [==============================] - 34s 4ms/step - loss: 0.1249 - acc: 0.9574 - val_loss: 0.2621 - val_acc: 0.8973\n",
      "Epoch 3/10\n",
      "8500/8500 [==============================] - 34s 4ms/step - loss: 0.0369 - acc: 0.9901 - val_loss: 0.3116 - val_acc: 0.8953\n",
      "Epoch 4/10\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.3576 - val_acc: 0.8973\n",
      "Epoch 5/10\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.3843 - val_acc: 0.9040\n",
      "Epoch 6/10\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 3.1293e-04 - acc: 1.0000 - val_loss: 0.4105 - val_acc: 0.9013\n",
      "Epoch 7/10\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 1.6474e-04 - acc: 1.0000 - val_loss: 0.4294 - val_acc: 0.9047\n",
      "Epoch 8/10\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 9.5026e-05 - acc: 1.0000 - val_loss: 0.4534 - val_acc: 0.9047\n",
      "Epoch 9/10\n",
      "8500/8500 [==============================] - 32s 4ms/step - loss: 5.6891e-05 - acc: 1.0000 - val_loss: 0.4725 - val_acc: 0.9060\n",
      "Epoch 10/10\n",
      "8500/8500 [==============================] - 31s 4ms/step - loss: 3.4185e-05 - acc: 1.0000 - val_loss: 0.4953 - val_acc: 0.9060\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.9060\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "model_embedding = Sequential()\n",
    "model_embedding.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen))\n",
    "model_embedding.add(layers.Conv1D(64, 5, activation='relu'))\n",
    "model_embedding.add(layers.GlobalMaxPooling1D())\n",
    "model_embedding.add(layers.Dense(10, activation='relu'))\n",
    "model_embedding.add(layers.Dense(1, activation='sigmoid'))\n",
    "model_embedding.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_embedding.summary()\n",
    "history = model_embedding.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model_embedding.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_embedding.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
