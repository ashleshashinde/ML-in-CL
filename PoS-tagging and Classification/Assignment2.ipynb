{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "ref:https://becominghuman.ai/part-of-speech-tagging-tutorial-with-the-keras-deep-learning-library-d7f93fa05537\n",
    "\n",
    "Data is split in to training set, testing set, and validation set. Each of these sets are then split into tags and feature vectors for each word.\n",
    "\n",
    "N-gram model is used where a window of previous 3 and next 2 words is considered.\n",
    "\n",
    "The feature set also consists of 3 characters prefix and suffix of each word \n",
    "\n",
    "The features are returned in a dictionary.\n",
    "\n",
    "This feature dictionary is then vectorized using the DictVectorizer\n",
    "\n",
    "The neural network model uses sigmoid activation function and the softmax activation is applied at the output layer\n",
    "An accuracy of 96.8% is observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import random\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "sentences = treebank.tagged_sents(tagset='universal')\n",
    "tags =[]\n",
    "for sentence in treebank.tagged_sents():\n",
    "    for word, tag in sentence:\n",
    "        tags.append(tag)\n",
    "tags=set(tags)\n",
    "#print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into Training set , Testing set and Validation set\n",
    "train_test_cutoff = int(.80 * len(sentences)) \n",
    "training_sentences = sentences[:train_test_cutoff]\n",
    "testing_sentences = sentences[train_test_cutoff:]\n",
    "train_val_cutoff = int(.0005 * len(training_sentences))\n",
    "validation_sentences = training_sentences[:train_val_cutoff]\n",
    "training_sentences = training_sentences[train_val_cutoff:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "def untag(tagged_sentence):\n",
    "    words=[]\n",
    "    for word, _ in tagged_sentence:\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "def add_basic_features(sentence_terms, index):\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'nb_terms': len(sentence_terms),\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_terms) - 1,\n",
    "        'is_capitalized': term[0].upper() == term[0],\n",
    "        'is_all_caps': term.upper() == term,\n",
    "        'is_all_lower': term.lower() == term,\n",
    "        'prefix-1': term[0],\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'suffix-1': term[-1],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],\n",
    "        'prev_word1': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'prev_word2': '' if index == 0 or index == 1 else sentence_terms[index - 2],\n",
    "        'prev_word3': '' if index == 0 or index == 1 or index == 2else sentence_terms[index - 3],\n",
    "        'next_word1': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1],\n",
    "        'next_word2': '' if index == len(sentence_terms) - 1 or index == len(sentence_terms) - 2 else sentence_terms[index + 2]\n",
    "    }\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for pos_tags in tagged_sentences:\n",
    "        for index, (term, class_) in enumerate(pos_tags):\n",
    "            X.append(add_basic_features(untag(pos_tags), index))\n",
    "            y.append(class_)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = transform_to_dataset(training_sentences)\n",
    "X_test, y_test = transform_to_dataset(testing_sentences)\n",
    "X_val, y_val = transform_to_dataset(validation_sentences)\n",
    "val_posts,val_tags = [],[]\n",
    "for pos_tags in validation_sentences:\n",
    "    for index, (term, class_) in enumerate(pos_tags):\n",
    "            val_posts.append(untag(pos_tags))\n",
    "            val_tags.append(class_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vectors\n",
    "\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "dict_vectorizer.fit(X_train + X_test + X_val)\n",
    "X_train = dict_vectorizer.transform(X_train)\n",
    "X_test = dict_vectorizer.transform(X_test)\n",
    "X_val = dict_vectorizer.transform(X_val)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train + y_test + y_val)\n",
    "\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_val = np_utils.to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72557 samples, validate on 8062 samples\n",
      "Epoch 1/3\n",
      "72557/72557 [==============================] - 1270s 17ms/step - loss: 0.6501 - acc: 0.8309 - val_loss: 0.1978 - val_acc: 0.9484\n",
      "Epoch 2/3\n",
      "72557/72557 [==============================] - 1201s 17ms/step - loss: 0.1283 - acc: 0.9667 - val_loss: 0.1225 - val_acc: 0.9651\n",
      "Epoch 3/3\n",
      "72557/72557 [==============================] - 1247s 17ms/step - loss: 0.0661 - acc: 0.9845 - val_loss: 0.0971 - val_acc: 0.9701\n",
      "20039/20039 [==============================] - 226s 11ms/step\n",
      "Test score: 0.09513067156635531\n",
      "Test accuracy: 0.9689106192499097\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=X_train.shape[1]))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(12))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=256, \n",
    "                    epochs=3, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, \n",
    "                       batch_size=10, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "text: Pierre\n",
      "Actual label:NOUN\n",
      "Predicted label: NOUN\n",
      "Probabilities of labels: [0.00361503 0.18973994 0.04224168 0.14558423 0.00351953 0.05036765\n",
      " 0.5019615  0.01056448 0.01418394 0.00225018 0.02883826 0.00713357]\n",
      "....\n",
      "text: Vinken\n",
      "Actual label:NOUN\n",
      "Predicted label: NOUN\n",
      "Probabilities of labels: [3.9041904e-04 2.6865397e-03 5.3626078e-04 7.5662555e-03 1.2549205e-04\n",
      " 1.8793099e-04 9.7823763e-01 1.6962015e-03 2.4471979e-04 2.2549546e-04\n",
      " 7.5228475e-03 5.8017467e-04]\n",
      "....\n",
      "text: ,\n",
      "Actual label:.\n",
      "Predicted label: .\n",
      "Probabilities of labels: [9.9797279e-01 2.5965649e-04 9.7500430e-05 2.3769135e-04 4.3621196e-05\n",
      " 7.0192844e-05 1.9327119e-04 4.3456105e-04 8.5083899e-05 8.3963067e-05\n",
      " 3.8148288e-04 1.4018241e-04]\n",
      "....\n",
      "text: 61\n",
      "Actual label:NUM\n",
      "Predicted label: NUM\n",
      "Probabilities of labels: [2.0273016e-03 3.9523072e-03 1.5476816e-03 1.9835664e-03 4.7425559e-04\n",
      " 9.2269573e-04 1.2278978e-03 9.7583789e-01 5.5424764e-04 7.8292296e-04\n",
      " 1.2641661e-03 9.4250720e-03]\n",
      "....\n",
      "text: years\n",
      "Actual label:NOUN\n",
      "Predicted label: NOUN\n",
      "Probabilities of labels: [5.69971598e-05 1.25660596e-03 1.27337145e-04 2.67488154e-04\n",
      " 3.32554082e-05 7.06289939e-05 9.97211039e-01 1.12080204e-04\n",
      " 1.63304197e-04 5.75590238e-05 5.85297472e-04 5.84380214e-05]\n",
      "....\n",
      "text: old\n",
      "Actual label:ADJ\n",
      "Predicted label: ADJ\n",
      "Probabilities of labels: [1.19931814e-04 9.87253249e-01 9.32997675e-04 1.63776567e-03\n",
      " 1.83511846e-04 1.27118939e-04 6.64013810e-03 6.66493783e-04\n",
      " 1.64847908e-04 2.12637649e-04 1.93333405e-03 1.27890613e-04]\n",
      "....\n",
      "text: ,\n",
      "Actual label:.\n",
      "Predicted label: .\n",
      "Probabilities of labels: [9.9792540e-01 1.3612524e-04 5.2724761e-05 1.4672427e-04 3.2354848e-05\n",
      " 4.0789619e-05 9.3807757e-04 3.2065899e-04 7.8477366e-05 6.7665140e-05\n",
      " 1.2254596e-04 1.3844848e-04]\n",
      "....\n",
      "text: will\n",
      "Actual label:VERB\n",
      "Predicted label: VERB\n",
      "Probabilities of labels: [2.1633488e-04 6.8232035e-03 1.2865646e-03 7.6058619e-03 1.5665741e-04\n",
      " 5.9925602e-04 5.1737023e-03 3.7115923e-04 4.8792336e-04 2.3639319e-04\n",
      " 9.7688872e-01 1.5417293e-04]\n",
      "....\n",
      "text: join\n",
      "Actual label:VERB\n",
      "Predicted label: VERB\n",
      "Probabilities of labels: [2.1416195e-04 2.1409048e-03 9.3400665e-03 7.1849059e-03 1.4571645e-04\n",
      " 9.1545924e-05 4.6528960e-04 4.4818746e-04 1.8164270e-04 8.1674266e-04\n",
      " 9.7880644e-01 1.6436596e-04]\n",
      "....\n",
      "text: the\n",
      "Actual label:DET\n",
      "Predicted label: DET\n",
      "Probabilities of labels: [4.53625726e-05 1.39998330e-03 8.48335185e-05 7.85063428e-04\n",
      " 1.06484495e-05 9.95960653e-01 1.99556700e-04 2.51442369e-04\n",
      " 7.54041888e-04 1.48941705e-04 2.53522740e-04 1.05899191e-04]\n",
      "....\n",
      "text: board\n",
      "Actual label:NOUN\n",
      "Predicted label: NOUN\n",
      "Probabilities of labels: [7.7634882e-05 4.8207953e-03 1.7822924e-04 1.7704915e-03 6.3335596e-05\n",
      " 1.2828021e-04 9.9164432e-01 3.2884232e-04 5.2792046e-05 7.3344068e-05\n",
      " 7.2083238e-04 1.4089025e-04]\n",
      "....\n",
      "text: as\n",
      "Actual label:ADP\n",
      "Predicted label: ADP\n",
      "Probabilities of labels: [1.20793426e-04 1.44032441e-04 9.42716062e-01 5.12539074e-02\n",
      " 1.57616450e-04 1.95071669e-04 1.09182368e-03 2.03666452e-04\n",
      " 1.31063513e-04 7.26033817e-04 3.12780798e-03 1.32247369e-04]\n",
      "....\n",
      "text: a\n",
      "Actual label:DET\n",
      "Predicted label: DET\n",
      "Probabilities of labels: [3.0972380e-05 4.7647223e-04 2.6619810e-04 2.0636275e-04 1.2713864e-05\n",
      " 9.9811077e-01 5.3874386e-04 8.4438114e-05 6.7502260e-05 7.7851117e-05\n",
      " 2.9901885e-05 9.8046374e-05]\n",
      "....\n",
      "text: nonexecutive\n",
      "Actual label:ADJ\n",
      "Predicted label: ADJ\n",
      "Probabilities of labels: [2.0755905e-04 9.3379766e-01 6.8210892e-04 2.8475798e-03 1.8579427e-04\n",
      " 1.1798055e-03 5.4878484e-02 4.3338370e-03 2.3597866e-04 1.8605703e-04\n",
      " 1.1659063e-03 2.9919817e-04]\n",
      "....\n",
      "text: director\n",
      "Actual label:NOUN\n",
      "Predicted label: NOUN\n",
      "Probabilities of labels: [1.7729684e-04 6.8372865e-03 1.8670411e-03 5.2446121e-04 3.0222349e-04\n",
      " 1.5460285e-04 9.8737234e-01 2.9368169e-04 1.9873980e-04 2.0015627e-04\n",
      " 1.9165981e-03 1.5562341e-04]\n",
      "....\n",
      "text: Nov.\n",
      "Actual label:NOUN\n",
      "Predicted label: NOUN\n",
      "Probabilities of labels: [6.0030748e-04 1.5651581e-03 3.2073012e-04 8.7740371e-04 1.6668593e-04\n",
      " 4.9946801e-04 9.9427235e-01 7.8825519e-04 1.3842070e-04 1.4987441e-04\n",
      " 1.4545748e-04 4.7599251e-04]\n",
      "....\n",
      "text: 29\n",
      "Actual label:NUM\n",
      "Predicted label: NUM\n",
      "Probabilities of labels: [5.0082435e-03 3.0408837e-03 7.2430808e-04 3.6944763e-03 3.0479403e-04\n",
      " 8.4923132e-04 1.9336773e-02 9.4616127e-01 6.2641030e-04 1.1379143e-03\n",
      " 1.0088171e-03 1.8106855e-02]\n",
      "....\n",
      "text: .\n",
      "Actual label:.\n",
      "Predicted label: .\n",
      "Probabilities of labels: [9.9920279e-01 6.7436915e-05 2.6015146e-05 9.5872419e-05 1.4961798e-05\n",
      " 1.7526245e-05 1.9911183e-04 1.6673059e-04 2.6296062e-05 3.6720772e-05\n",
      " 8.5441316e-05 6.1080093e-05]\n"
     ]
    }
   ],
   "source": [
    "#validating the output\n",
    "for i in range(len(X_val)):    \n",
    "    prediction = model.predict(np.array([X_val[i]]))\n",
    "    text_labels = label_encoder.classes_ \n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    prob = prediction[0]\n",
    "    print(\"....\")\n",
    "    print(\"text: \"+str(val_posts[0][i]))\n",
    "    print('Actual label:' + val_tags[i])\n",
    "    print(\"Predicted label: \" + predicted_label)  \n",
    "    print(\"Probabilities of labels: \" + str(prob)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "ref:https://cloud.google.com/blog/products/gcp/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts\n",
    "\n",
    "The corpus is created by merging text tagged as greeitng, goodbye, and action directive from The nps chats corpus and The Switchboard Dialog Act Corpus\n",
    "\n",
    "example of data from the corpus:<br>\n",
    "  greeting: hola<br>\n",
    "  goodbye: ok..i'm gone...again...cya later <br>\n",
    "  request: Can you hang on just a minute? \n",
    "  \n",
    "The data is loaded from the corpus into a list of dictionaries (with keys: class & text and values: dialogue act  & text)\n",
    "This list of dictionaries is split into training set, testing set and validation set and then, each these sets are split into seperate text and tags list which are then converted to vectors using the bag of words model.\n",
    "\n",
    "A sequential model is used with relu activation function in the hidden layers and the softmax activation in the output layer.\n",
    "\n",
    "Test accuracy or 92.6% is observed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loading and Preprocessing\n",
    "#Loading the Data\n",
    "data = []\n",
    "post=[]\n",
    "tag=[]\n",
    "words=[]\n",
    "with open(\"myCorpus.txt\",\"r\") as corpus:\n",
    "    for line in corpus:\n",
    "        label=line.split(\":\", 1)[0]        \n",
    "        text=line.split(\":\", 1)[1]\n",
    "        data.append({\"class\":label,\"sentence\":text})\n",
    "        for word in text.split():\n",
    "            if word not in words:\n",
    "                words.append(word)\n",
    " \n",
    "words=list(set(words))\n",
    "\n",
    "#splitting it into Training set , Testing set and Validation set\n",
    "random.shuffle(data)\n",
    "train_size = int(len(data) * .8)\n",
    "train=data[:train_size]\n",
    "test=data[train_size:]\n",
    "val_size=int(.002*len(train))\n",
    "validation=train[:val_size]\n",
    "train=train[val_size:]\n",
    "\n",
    "train_posts=[]\n",
    "train_tags=[]\n",
    "for post in train:\n",
    "    train_posts.append(post['sentence'])\n",
    "    train_tags.append(post['class'])\n",
    "    \n",
    "test_posts = []\n",
    "test_tags = []\n",
    "for post in test:\n",
    "    test_posts.append(post['sentence'])\n",
    "    test_tags.append(post['class'])\n",
    "\n",
    "val_posts=[]\n",
    "val_tags=[]\n",
    "for post in validation:\n",
    "    val_posts.append(post['sentence'])\n",
    "    val_tags.append(post['class'])\n",
    "\n",
    "#creating vectors for each set using bag or words model \n",
    "vocab_size = len(words)\n",
    "tokenize = Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(train_posts)\n",
    "    \n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "x_val = tokenize.texts_to_matrix(val_posts)\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_val = encoder.transform(val_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1608 samples, validate on 179 samples\n",
      "Epoch 1/3\n",
      "1608/1608 [==============================] - 10s 6ms/step - loss: 0.5221 - acc: 0.8576 - val_loss: 0.2512 - val_acc: 0.9162\n",
      "Epoch 2/3\n",
      "1608/1608 [==============================] - 8s 5ms/step - loss: 0.1172 - acc: 0.9664 - val_loss: 0.2051 - val_acc: 0.9218\n",
      "Epoch 3/3\n",
      "1608/1608 [==============================] - 7s 5ms/step - loss: 0.0497 - acc: 0.9894 - val_loss: 0.1892 - val_acc: 0.9330\n",
      "448/448 [==============================] - 0s 888us/step\n",
      "Test score: 0.2229078364315293\n",
      "Test accuracy: 0.9263392761349678\n"
     ]
    }
   ],
   "source": [
    "#Building a model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=10, \n",
    "                    epochs=3, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, \n",
    "                       batch_size=10, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "text:  hi \n",
      "\n",
      "Actual label:greeting\n",
      "Predicted label: greeting\n",
      "....\n",
      "text:  you want to go ahead and tell me your favorite team, or  who you think will be doing well this year. \n",
      "\n",
      "Actual label:request\n",
      "Predicted label: request\n",
      "....\n",
      "text:  Byebye. \n",
      "\n",
      "Actual label:goodbye\n",
      "Predicted label: goodbye\n",
      "....\n",
      "text:  Okay. \n",
      "\n",
      "Actual label:goodbye\n",
      "Predicted label: request\n",
      "....\n",
      "text:  nice talking to you, Linda. \n",
      "\n",
      "Actual label:goodbye\n",
      "Predicted label: request\n",
      "....\n",
      "text:  hey  \n",
      "\n",
      "Actual label:greeting\n",
      "Predicted label: request\n",
      "....\n",
      "text:  Hi  sweetie\n",
      "\n",
      "Actual label:greeting\n",
      "Predicted label: greeting\n",
      "....\n",
      "text:   Me too.  \n",
      "\n",
      "Actual label:goodbye\n",
      "Predicted label: request\n"
     ]
    }
   ],
   "source": [
    "#Validating results\n",
    "for i in range(len(x_val)):    \n",
    "    prediction = model.predict(np.array([x_val[i]]))\n",
    "    text_labels = encoder.classes_ \n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    print(\"....\")\n",
    "    print(\"text: \"+val_posts[i])\n",
    "    print('Actual label:' + val_tags[i])\n",
    "    print(\"Predicted label: \" + predicted_label)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
